{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cifar 10 example\n",
    "\n",
    "## Tested on GCP n2 series, provsioned with 4 Vcpu and 8 GB / container "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jax==0.4.1\n",
      "  Downloading jax-0.4.1.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jaxlib==0.4.1\n",
      "  Downloading jaxlib-0.4.1-cp38-cp38-manylinux2014_x86_64.whl (71.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pendulum==2.1.2\n",
      "  Downloading pendulum-2.1.2-cp38-cp38-manylinux1_x86_64.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow==2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.24.0\n",
      "  Downloading numpy-1.24.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.5.2\n",
      "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyvis==0.3.1\n",
      "  Downloading pyvis-0.3.1.tar.gz (748 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m748.9/748.9 KB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting plotly==5.11.0\n",
      "  Downloading plotly-5.11.0-py2.py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib==3.6.2\n",
      "  Downloading matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-text==2.11.0\n",
      "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting imageio==2.25.0\n",
      "  Downloading imageio-2.25.0-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt_einsum\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax==0.4.1->-r requirements.txt (line 1)) (1.6.3)\n",
      "Collecting pytzdata>=2020.1\n",
      "  Downloading pytzdata-2020.1-py2.py3-none-any.whl (489 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.0/490.0 KB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0,>=2.6 in /usr/local/lib/python3.8/dist-packages (from pendulum==2.1.2->-r requirements.txt (line 3)) (2.8.2)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 4)) (1.50.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.30.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 4)) (62.0.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 4)) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 4)) (2.0.1)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 4)) (21.3)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->-r requirements.txt (line 4)) (3.10.0.2)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.2->-r requirements.txt (line 6)) (2022.5)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.8/dist-packages (from pyvis==0.3.1->-r requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.8/dist-packages (from pyvis==0.3.1->-r requirements.txt (line 7)) (2.6.3)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from pyvis==0.3.1->-r requirements.txt (line 7)) (8.5.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from pyvis==0.3.1->-r requirements.txt (line 7)) (2.2.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.1-py3-none-any.whl (24 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.6.2->-r requirements.txt (line 9)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.6.2->-r requirements.txt (line 9)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.6.2->-r requirements.txt (line 9)) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.6.2->-r requirements.txt (line 9)) (3.0.9)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 KB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0->-r requirements.txt (line 4)) (0.30.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (3.0.31)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (0.1.6)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (2.13.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.9.6->pyvis==0.3.1->-r requirements.txt (line 7)) (2.1.1)\n",
      "Collecting scipy>=1.5\n",
      "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (2.28.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (1.34.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 KB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (5.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (1.26.12)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (2.0.8)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.1->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 4)) (3.2.2)\n",
      "Building wheels for collected packages: jax, pyvis\n",
      "  Building wheel for jax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.4.1-py3-none-any.whl size=1338540 sha256=df64ef59f96e3687f57701fe2c8d2e08a09b47f2b6fe8219bb2a7e4a9b86e28e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e2/c2/cc/ac766a5c8ed28aec746fc292690d3e2e3790b554d2a6abacb7\n",
      "  Building wheel for pyvis (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyvis: filename=pyvis-0.3.1-py3-none-any.whl size=756330 sha256=14ec7e8c7728b13dfeaf3e2539c53a8382b705a823d1ea58feda5fd355a25a56\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a4/0c/61/8469ca276f96ab772c3acc7f47d71e9737cbdf6f446f017f48\n",
      "Successfully built jax pyvis\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, werkzeug, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, tenacity, pytzdata, protobuf, numpy, keras, google-pasta, gast, fonttools, astunparse, absl-py, tensorflow-hub, scipy, plotly, pendulum, pandas, opt_einsum, markdown, imageio, h5py, contourpy, matplotlib, jaxlib, jax, google-auth-oauthlib, tensorboard, pyvis, tensorflow, tensorflow-text\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.3 requires numpy<1.20,>=1.16.0, but you have numpy 1.24.0 which is incompatible.\n",
      "ml-metadata 0.30.0 requires absl-py<0.13,>=0.9, but you have absl-py 1.4.0 which is incompatible.\n",
      "kubeflow-kale 2.0rc3.post0+6.g3f6dfd87b requires matplotlib<3.4.0, but you have matplotlib 3.6.2 which is incompatible.\n",
      "kubeflow-kale 2.0rc3.post0+6.g3f6dfd87b requires scipy<1.7, but you have scipy 1.10.0 which is incompatible.\n",
      "kserve 0.8.0 requires numpy~=1.19.2, but you have numpy 1.24.0 which is incompatible.\n",
      "kfp 1.7.post0+41.gb3589b6c6 requires absl-py<=0.11,>=0.9, but you have absl-py 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 astunparse-1.6.3 contourpy-1.0.7 flatbuffers-23.1.21 fonttools-4.38.0 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.8.0 imageio-2.25.0 jax-0.4.1 jaxlib-0.4.1 keras-2.11.0 libclang-15.0.6.1 markdown-3.4.1 matplotlib-3.6.2 numpy-1.24.0 opt_einsum-3.3.0 pandas-1.5.2 pendulum-2.1.2 plotly-5.11.0 protobuf-3.19.6 pytzdata-2020.1 pyvis-0.3.1 scipy-1.10.0 tenacity-8.2.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-hub-0.12.0 tensorflow-io-gcs-filesystem-0.30.0 tensorflow-text-2.11.0 werkzeug-2.2.3\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Mark in Kale as \"skip cell\", but run this and the next cell first\n",
    "# Install required packages\n",
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mark in Kale as \"skip cell\", but run this after the last cell first\n",
    "# Restart the kernel to apply the updates\n",
    "import sys\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 20:56:47.432550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-14 20:56:47.559367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-14 20:56:47.559388: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-14 20:56:48.371520: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-14 20:56:48.371628: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-14 20:56:48.371636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-14 20:56:50.431605: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-14 20:56:50.431639: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-14 20:56:50.431657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (test-images-0): /proc/driver/nvidia/version does not exist\n",
      "2023-02-14 20:56:50.431858: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Mark in Kale as Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio.v3 as iio\n",
    "# from multiprocessing import Pool  # , Process\n",
    "from cerebros.simplecerebrosrandomsearch.simple_cerebros_random_search\\\n",
    "    import SimpleCerebrosRandomSearch\n",
    "import pendulum\n",
    "from cerebros.units.units import DenseUnit\n",
    "from cerebros.denseautomlstructuralcomponent.dense_automl_structural_component\\\n",
    "    import zero_7_exp_decay, zero_95_exp_decay, simple_sigmoid\n",
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Mark in Kale as Pipeline params:\n",
    "\n",
    "activation = 'relu'\n",
    "predecessor_level_connection_affinity_factor_first = 2.0\n",
    "predecessor_level_connection_affinity_factor_main = 0.97\n",
    "max_consecutive_lateral_connections = 5\n",
    "p_lateral_connection = 0.97\n",
    "num_lateral_connection_tries_per_unit = 2\n",
    "learning_rate = 0.001\n",
    "epochs = 10  # [1, 100]\n",
    "batch_size = 20\n",
    "maximum_levels = 4  # [3,7]\n",
    "maximum_units_per_level = 7  # [2,10]\n",
    "maximum_neurons_per_unit = 4  # [2,20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:data_ingest"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Mark in Kale as Pipeline step data_ingest\n",
    "\n",
    "\n",
    "### Global configurables:\n",
    "\n",
    "INPUT_SHAPES = [(32, 32, 3)]  # resize from ]\n",
    "RESIZE_TO = (224, 224, 3)\n",
    "OUTPUT_SHAPES = [10]\n",
    "\n",
    "# Read in the data set and make it useable\n",
    "\n",
    "ciphar10_metadata = pd.read_csv(\"cifar10-mini/file_metadata.csv\")\n",
    "\n",
    "ciphar10_train = ciphar10_metadata.query(\"data_set == 'train'\")\n",
    "ciphar10_test = ciphar10_metadata.query(\"data_set == 'test'\")\n",
    "\n",
    "def make_dataset(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in np.arange(ciphar10_metadata.shape[0]):\n",
    "        imfile = ciphar10_metadata.loc[i]['file_name']\n",
    "\n",
    "        # Debug delete\n",
    "        # print(f\"$$$$: attempting file: {imfile}\")\n",
    "\n",
    "        img = iio.imread(imfile)\n",
    "\n",
    "        images.append(np.array(img))\n",
    "        labels.append(int(ciphar10_metadata.loc[i]['label']))\n",
    "    data_tensor = tf.constant(images)\n",
    "    labels_tensor = tf.constant(labels)\n",
    "    labels_tensor_ohe = tf.one_hot(indices=labels_tensor,\n",
    "                                   depth=10)\n",
    "    print(f\"labels_tensor_ohe shape: {labels_tensor_ohe.shape}\")\n",
    "    print(f\"data_tensor shape: {data_tensor.shape}\")\n",
    "    return data_tensor, labels_tensor_ohe\n",
    "\n",
    "\n",
    "selected_x_train, selected_y_train_ohe =\\\n",
    "    make_dataset(ciphar10_train)\n",
    "\n",
    "training_x = [selected_x_train]\n",
    "train_labels = [selected_y_train_ohe]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:train",
     "prev:data_ingest"
    ]
   },
   "outputs": [],
   "source": [
    "# Mark in Kale as Pipeline step train; Requires data_ingest\n",
    "\n",
    "base_new = tf.keras.applications.MobileNetV3Large(\n",
    "    input_shape=None,\n",
    "    alpha=1.0,\n",
    "    minimalistic=False,\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    classes=1000,\n",
    "    pooling=None,\n",
    "    dropout_rate=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    include_preprocessing=True,\n",
    ")\n",
    "\n",
    "for layer in base_new.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "last_relevant_layer = base_new.layers[-2]\n",
    "# last_relevant_layer_extracted = last_relevant_layer #.output[0][0][0]\n",
    "base_embedding = tf.keras.Model(inputs=base_new.layers[0].input,\n",
    "                                outputs=last_relevant_layer.output)\n",
    "\n",
    "\n",
    "image_input_0 = tf.keras.layers.Input(shape=INPUT_SHAPES[0])\n",
    "resizing = tf.keras.layers.Resizing(\n",
    "    height=RESIZE_TO[0],\n",
    "    width=RESIZE_TO[1],\n",
    "    interpolation='bilinear',\n",
    "    crop_to_aspect_ratio=False)\n",
    "resized = resizing(image_input_0)\n",
    "embedded = base_embedding(resized)\n",
    "\n",
    "embedding_model = tf.keras.Model(image_input_0,\n",
    "                                 embedded)\n",
    "\n",
    "# Final training task\n",
    "\n",
    "TIME = pendulum.now(tz='America/New_York').__str__()[:16]\\\n",
    "    .replace('T', '_')\\\n",
    "    .replace(':', '_')\\\n",
    "    .replace('-', '_')\n",
    "PROJECT_NAME = f'{TIME}_cerebros_auto_ml_test'\n",
    "\n",
    "meta_trial_number = str(int(np.random.random() * 10 ** 12))\n",
    "\n",
    "cerebros_automl = SimpleCerebrosRandomSearch(\n",
    "    unit_type=DenseUnit,\n",
    "    input_shapes=INPUT_SHAPES,\n",
    "    output_shapes=OUTPUT_SHAPES,\n",
    "    training_data=training_x,\n",
    "    labels=train_labels,\n",
    "    validation_split=0.35,\n",
    "    direction='maximize',\n",
    "    metric_to_rank_by=\"val_top_1_categorical_accuracy\",\n",
    "    minimum_levels=2,\n",
    "    maximum_levels=maximum_levels,\n",
    "    minimum_units_per_level=1,\n",
    "    maximum_units_per_level=maximum_units_per_level,\n",
    "    minimum_neurons_per_unit=1,\n",
    "    maximum_neurons_per_unit=maximum_neurons_per_unit,\n",
    "    activation=activation,\n",
    "    final_activation='softmax',\n",
    "    number_of_architecture_moities_to_try=2,\n",
    "    number_of_tries_per_architecture_moity=1,\n",
    "    minimum_skip_connection_depth=1,\n",
    "    maximum_skip_connection_depth=7,\n",
    "    predecessor_level_connection_affinity_factor_first=predecessor_level_connection_affinity_factor_first,\n",
    "    predecessor_level_connection_affinity_factor_first_rounding_rule='ceil',\n",
    "    predecessor_level_connection_affinity_factor_main=predecessor_level_connection_affinity_factor_main,\n",
    "    predecessor_level_connection_affinity_factor_main_rounding_rule='ceil',\n",
    "    predecessor_level_connection_affinity_factor_decay_main=zero_7_exp_decay,\n",
    "    seed=8675309,\n",
    "    max_consecutive_lateral_connections=max_consecutive_lateral_connections,\n",
    "    gate_after_n_lateral_connections=3,\n",
    "    gate_activation_function=simple_sigmoid,\n",
    "    p_lateral_connection=p_lateral_connection,\n",
    "    p_lateral_connection_decay=zero_95_exp_decay,\n",
    "    num_lateral_connection_tries_per_unit=num_lateral_connection_tries_per_unit,\n",
    "    learning_rate=learning_rate,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.TopKCategoricalAccuracy(\n",
    "                k=1, name='top_1_categorical_accuracy')\n",
    "             ],\n",
    "    epochs=epochs,\n",
    "    project_name=f\"{PROJECT_NAME}_meta_{meta_trial_number}\",\n",
    "    # use_multiprocessing_for_multiple_neural_networks=False,  # pull this param\n",
    "    model_graphs='model_graphs',\n",
    "    batch_size=batch_size,\n",
    "    meta_trial_number=meta_trial_number,\n",
    "    base_models=[embedding_model])\n",
    "val_top_1_categorical_accuracy =\\\n",
    "    cerebros_automl.run_random_search()\n",
    "# Mark in Kale as Pipeline metrics Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pipeline-metrics"
    ]
   },
   "outputs": [],
   "source": [
    "print(val_top_1_categorical_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "deploy_config": {},
   "docker_image": "",
   "experiment_name": "image-classification-b",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "bayesianoptimization",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "10"
      },
      {
       "name": "acq_optimizer",
       "value": "auto"
      },
      {
       "name": "acq_func",
       "value": "gp_hedge"
      },
      {
       "name": "base_estimator",
       "value": "GP"
      }
     ]
    },
    "maxFailedTrialCount": 10,
    "maxTrialCount": 40,
    "objective": {
     "additionalMetricNames": [],
     "goal": 1,
     "objectiveMetricName": "val-top-1-categorical-accuracy",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "list": [
        "relu",
        "elu",
        "gelu"
       ]
      },
      "name": "activation",
      "parameterType": "categorical"
     },
     {
      "feasibleSpace": {
       "max": "40",
       "min": "1",
       "step": "0.1"
      },
      "name": "predecessor_level_connection_affinity_factor_first",
      "parameterType": "double"
     },
     {
      "feasibleSpace": {
       "max": "65",
       "min": "0.1",
       "step": "0.1"
      },
      "name": "predecessor_level_connection_affinity_factor_main",
      "parameterType": "double"
     },
     {
      "feasibleSpace": {
       "max": "40",
       "min": "2",
       "step": "1"
      },
      "name": "max_consecutive_lateral_connections",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "max": "50",
       "min": "0.1",
       "step": "0.1"
      },
      "name": "p_lateral_connection",
      "parameterType": "double"
     },
     {
      "feasibleSpace": {
       "max": "40",
       "min": "1",
       "step": "1"
      },
      "name": "num_lateral_connection_tries_per_unit",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "max": "0.3",
       "min": "0.00001",
       "step": "0.00001"
      },
      "name": "learning_rate",
      "parameterType": "double"
     },
     {
      "feasibleSpace": {
       "max": "7",
       "min": "1",
       "step": "1"
      },
      "name": "epochs",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "max": "40",
       "min": "5",
       "step": "1"
      },
      "name": "batch_size",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "max": "8",
       "min": "2"
      },
      "name": "maximum_levels",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "max": "10",
       "min": "2",
       "step": "1"
      },
      "name": "maximum_units_per_level",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "max": "10",
       "min": "2",
       "step": "1"
      },
      "name": "maximum_neurons_per_unit",
      "parameterType": "int"
     }
    ]
   },
   "katib_run": true,
   "pipeline_description": "Cifar 10 example",
   "pipeline_name": "image-classification-b",
   "snapshot_volumes": true,
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "test-images-workspace-pmbn9",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
